{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ex 2.1 Hadoop MapReduce with Python\n",
    "There are two prominent *Python* APIs for interfacing *Hadoop MapReduce* clusters:\n",
    "\n",
    "## *Snakebite* for *HDFS* access\n",
    "The [Snakebite Lib](https://github.com/spotify/snakebite) allows easy access to *HDFS* file systems:  \n",
    "```\n",
    ">>> from snakebite.client import Client\n",
    ">>> client = Client(\"localhost\", 8020, use_trash=False)\n",
    ">>> for x in client.ls(['/']):\n",
    "...     print x\n",
    "```\n",
    "\n",
    "See [documentation](https://snakebite.readthedocs.io/en/latest/) for details.\n",
    "\n",
    "\n",
    "## *MRJOB* for *MapReduce* job execution\n",
    "The ``mrjob`` lib -> [see docu](https://mrjob.readthedocs.io/en/latest/index.html) is a power full *MapReduce* client for *Python*. Some of the key features are:\n",
    "\n",
    "* local emulation (single and multi-core) a *Hadoop* cluster for development and debugging\n",
    "* simple access, authentication and file transfer to *Hadoop* clusters\n",
    "* powerful API for common cloud services, such as AWS or Azure   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing our environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#install mrjob lib and boto3 for AWS S3 access\n",
    "!conda install -c conda-forge -y mrjob boto3\n",
    "\n",
    "#or !pip install mrjob boto3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A *MRJOB* Example: WordCount (again)\n",
    "Since *Hadoop* works only on file in- and outputs, we do not have usual function based API. We need to pass our code (implementation of *Map* and *Reduce*) as executable *Python* scripts:\n",
    "\n",
    "* use *Jupyter's* ``%%file`` magic command to write the cell to file\n",
    "* create a executable script with ``__main__`` method\n",
    "* inherit from the ``MRJob`` class\n",
    "* implement ``mapper()`` and ``reducer()`` methods\n",
    "* call ``run()`` at start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%file wordcount.py \n",
    "#this will save this cell as file\n",
    "\n",
    "from mrjob.job import MRJob\n",
    "\n",
    "class MRWordCount(MRJob):\n",
    "    def mapper(self, _, line):\n",
    "        for word in line.split():\n",
    "            yield(word, 1)\n",
    " \n",
    "    def reducer(self, word, counts):\n",
    "        yield(word, sum(counts))\n",
    "        \n",
    "if __name__ == '__main__':\n",
    "    MRWordCount.run()\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### execute script from cmd\n",
    "* ``-r local`` causes local multi-core emulation a *Hadoop* cluster.\n",
    "* Input files are cmd arguments\n",
    "* define ouput-file (see docs) or use streams: `` > out.txt``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "! python wordcount.py -r local *.rst > out.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " -> results in **out.txt** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Execution on AWS EMR\n",
    "AWS EMR is a clound formation service which allows you to create *Hadoop*, *Spark* and other data analytics clusters with a few clicks.\n",
    "\n",
    "**NOTE**: we are not endorsing AWS specifically, other cloud service providers have similar offers\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Connect to existing cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%file mrjob_cluster.conf\n",
    "runners:\n",
    "  emr:\n",
    "    aws_access_key_id: YOUR_KEY_ID\n",
    "    aws_secret_access_key: YOUR_KEY_SECRET\n",
    "    region: eu-west-1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need the **ID** of the cluster we want to connect to - here pre-set to our Cluster today"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! python wordcount.py -r emr --cluster-id=j-L1BO0NYZIYY0 text1.rst text2.rst -c mrjob_cluster.conf  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise\n",
    "Use  *mrjob*  to  compute  employee  **top  annual  salaries** and  **gross pay** in the *CSV* table ``Baltimore_City_employee_Salaries_FY2014.csv``.\n",
    "\n",
    "* use  ``import csv`` to read the data -> [API docs](https://docs.python.org/3/library/csv.html)\n",
    "* use ``yield`` to return *producers* from *map* and *reduce* functions\n",
    "* return top entries in both categories "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
